{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this first: pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save webpage content to a CSV file\n",
    "def save_to_csv(title, content, url, csv_file):\n",
    "    # Open the CSV file in append mode\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([orgId, title, content, url])\n",
    "\n",
    "# Function to scrape and save webpage content\n",
    "def scrape_and_save(url, csv_file):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {\n",
    "        'User-Agent': 'Chrome/90.0.4430.93'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract title of the page\n",
    "        title = soup.title.string if soup.title else \"No Title\"\n",
    "        # Extract text content of the page\n",
    "        content = soup.get_text()\n",
    "        # Remove leading/trailing whitespace and replace newlines with spaces\n",
    "        content = ' '.join(content.strip().split('\\n'))\n",
    "        # Save data to CSV file\n",
    "        save_to_csv(title, content, url, csv_file)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "# Function to crawl a website\n",
    "def crawl_website(original_url, start_url, depth, csv_file, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if start_url in visited:\n",
    "        return\n",
    "    \n",
    "    visited.add(start_url)\n",
    "\n",
    "    try:\n",
    "        # print(\"Sleeping for 1 second...\")\n",
    "        time.sleep(1)\n",
    "        # Crawl the start URL\n",
    "        if not is_same_domain(original_url, start_url):\n",
    "            print(f\"Skipping {start_url}\")\n",
    "            return\n",
    "            \n",
    "        print(\"Crawling URL:\", start_url)\n",
    "        scrape_and_save(start_url, csv_file)\n",
    "\n",
    "        if depth > 0:\n",
    "            # Send a GET request to the URL\n",
    "            headers = {\n",
    "                'User-Agent': 'Chrome/90.0.4430.93'\n",
    "            }\n",
    "            response = requests.get(start_url,headers=headers)\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Find all links on the page\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                absolute_url = link['href']\n",
    "                # Recursively crawl the links\n",
    "                crawl_website(original_url, absolute_url, depth - 1, csv_file, visited)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to crawl {start_url}: {e}\")\n",
    "\n",
    "def is_same_domain(original_url, new_url):\n",
    "    try:\n",
    "        original_domain = urlparse(original_url).netloc\n",
    "        new_domain = urlparse(new_url).netloc\n",
    "        return original_domain == new_domain\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing URLs: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = [\n",
    "    \"https://www.healthline.com/health/fertility/fertility-monitor\",\n",
    "    \"https://kegg.tech/blogs/learn/mira-vs-kegg-what-s-the-best-fertility-tracker\",\n",
    "    \"https://www.mmnfp.com/mira\",\n",
    "    \"https://www.innerbody.com/mira-review\",\n",
    "    \"https://casadesante.com/blogs/wellness/ava-vs-mira-vs-ovusense\",\n",
    "    \"https://finvsfin.com/ava-vs-mira-vs-ovusense/\",\n",
    "    \"https://www.healthline.com/health/birth-control/the-daysy-birth-control-and-fertility-tracker-review-2022\",\n",
    "    \"https://www.health.com/best-fertility-monitors-6951641\",\n",
    "    \"https://payitforwardfertility.org/best-fertility-monitor/\",\n",
    "    ]\n",
    "depth = 0\n",
    "csv_file = \"industry_fertility.csv\"\n",
    "orgId = 'mira'\n",
    "\n",
    "# Open the CSV file and write the header row\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['organization_id', 'title', 'content', 'url'])\n",
    "for start_url in start_urls:\n",
    "    crawl_website(start_url, start_url, depth, csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
